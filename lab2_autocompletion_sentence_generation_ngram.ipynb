{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocomplétion et génération de phrases avec des modèles de langue n-grammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.data.path.append('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1.  Chargement et prétraitement des données (12 points)\n",
    "\n",
    "<a name='1.1'></a>\n",
    "### 1.1 Chargement des données (1 point)\n",
    "\n",
    "Les données que vous allez utiliser dans ce travail sont contenues dans le fichier [trump.txt](./trump.txt)\n",
    "\n",
    "Lisez le contenu de ce fichier et stockez-le dans une variable data.\n",
    "\n",
    "Affichez ensuite les 300 premiers caractères\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you very much.\\nWe had an amazing convention.\\nThat was one of the best.\\nI think it was one of the best ever.\\nIn terms -- in terms of enthusiasm, in terms of I think what it represents, getting our word out.\\nIvanka was incredible last night.\\nShe did an incredible job.\\nAnd so many of the speakers'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"trump.txt\", \"r\", encoding='utf-8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "display(data[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "### 1.2  Segmentation (2 points)\n",
    "\n",
    "Pré-traitez les données en suivant les étapes suivantes:\n",
    "\n",
    "1. Enlever les majuscules.\n",
    "2. Remplacer les \"\\n\" par des espaces\n",
    "3. Séparer les données en phrases en utilisant les délimiteurs suivants `.`, `?` et `!` comme séparateur.\n",
    "4. Enlever les signes de ponctuation (Attention de garder les espaces).\n",
    "5. Enlever les phrases vides.\n",
    "6. Segmenter les phrases avec la fonction nltk.word_tokenize()\n",
    "\n",
    "Utilisez ensuite votre fonction pour prétraiter le jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(data):\n",
    "    \n",
    "    data = data.lower()\n",
    "    \n",
    "    sentences = re.split(r'\\.|\\?|\\!', data.replace('\\n', ' '))\n",
    "\n",
    "    sentences = [re.sub('[^A-Za-z0-9 ]+', '', s).strip() for s in sentences]\n",
    "\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "\n",
    "    sentences = [nltk.word_tokenize(sentence) for sentence in sentences] \n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cats', 'are', 'independent'], ['dogs', 'are', 'faithful']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "x = \"Cats are independent.\\nDogs are faithful.\"\n",
    "preprocess(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "\n",
    "```CPP\n",
    "[['cats', 'are', 'independent'], ['dogs', 'are', 'faithful']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.3'></a>\n",
    "###  1.3 Création d'ensembles d'entraînement et de test (1 point)\n",
    "\n",
    "Échantilloner de manière **aléatoire** 80% des données pour l'ensemble d'entrainement. Garder 20% pour l'ensemble de test. Utilisez la fonction train_test_split de sklearn. Stocker les résultats dans des variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Construction du vocabulaire (4 points)\n",
    "\n",
    "Comme dans le TP1, construisez un vocabulaire à partir des données d'entraînement. Vous pouvez reprendre votre code du TP1.\n",
    "\n",
    "Complétez la fonction **build_voc** qui retourne une liste de jetons qui sont présents au moins n fois (threshold passé en paramètre) dans la liste d'exemples (également passée en paramètre). Vous pouvez utiliser la classe collections.Counter.\n",
    "\n",
    "Ensuite, appelez cette fonction pour construire votre vocabulaire à partir de l'ensemble d'entraînement **en utilisant threshold=2**. Imprimez la taille du vocabulaire.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5597\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "def build_voc(documents, threshold):\n",
    "    vocabulary = Counter([word for sentence in documents for word in sentence ])\n",
    "    return set([word for word, n_occurence in vocabulary.items() if n_occurence >= threshold])\n",
    "\n",
    "voc = build_voc(train_data, 2)\n",
    "\n",
    "print(len(voc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.4'></a>\n",
    "### 1.5 Mots hors vocabulaire (4 points)\n",
    "\n",
    "Si votre modèle réalise de l'autocomplétion, mais qu'il rencontre un mot qu'il n'a jamais vu lors de l'entraînement, le modèle ne pourra donc pas prédire le mot suivant car il n'y a pas d'occurrence pour le mot actuel.\n",
    "\n",
    "Ces mots sont appelés les mots hors vocabulaire (Out of Vocabulary) <b>OOV</b>.\n",
    "Le pourcentage de mots inconnus dans l'ensemble de test est appelé le taux de mots <b> OOV </b>.\n",
    "\n",
    "Pour gérer les mots inconnus lors de la prédiction, utilisez un jeton spécial 'unk' pour représenter tous les mots inconnus. Plus spécifiquement, la technique que vous utiliserez sera la suivante:\n",
    "\n",
    "Complétez la fonction replace_oov qui convertit tous les mots qui ne font pas partie du vocabulaire en jeton '\\<unk\\>'.\n",
    "\n",
    "Appelez ensuite votre fonction sur votre corpus d'entraînement et de test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def replace_oov(tokenized_sentences, voc):\n",
    "    \n",
    "    new_data = [[word if word in voc else '<unk>' for word in sentence] for sentence in tokenized_sentences]\n",
    "    \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = replace_oov(train_data, voc)\n",
    "test_data = replace_oov(test_data, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase initiale:\n",
      "[['cats', 'sleep'], ['mice', 'eat'], ['cats', 'and', 'mice']]\n",
      "Phrase segmentée avec'<unk>':\n",
      "[['cats', '<unk>'], ['mice', '<unk>'], ['cats', '<unk>', 'mice']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]]\n",
    "vocabulary = build_voc([[\"cats\", \"sleep\"], [\"mice\", \"eat\"], [\"cats\", \"and\", \"mice\"]], 2)\n",
    "tmp_replaced_tokenized_sentences = replace_oov(tokenized_sentences, vocabulary)\n",
    "print(f\"Phrase initiale:\")\n",
    "print(tokenized_sentences)\n",
    "print(f\"Phrase segmentée avec'<unk>':\")\n",
    "print(tmp_replaced_tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "```CPP\n",
    "Phrase initiale:\n",
    "[['cats', 'sleep'], ['mice', 'eat'], ['cats', 'and', 'mice']]\n",
    "Phrase segmentée avec '<unk>':\n",
    "[['cats', '<unk>'], ['mice', '<unk>'], ['cats', '<unk>', 'mice']]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2. Modèles de langue n-gramme (18 points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette section, vous développerez un modèle de langue n-grammes. Nous allons utiliser la formule: \n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2} $$\n",
    "\n",
    "- La fonction $C(\\cdots)$ représente le nombre d'occurrences de la séquence donnée.\n",
    "- $\\hat{P}$ signifie l'estimation de $P$.\n",
    "\n",
    "Vous pouvez estimer cette probabilité en comptant les occurrences de ces séquences de mots dans les données d'entraînement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 Fréquence des n-grammes (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Vous allez commencer par mettre en œuvre une fonction qui calcule la fréquence des n-grammes pour un nombre arbitraire $n$.\n",
    "\n",
    "Vous devez pré-traiter la phrase en ajoutant $n-1$ marqueurs de début de phrase \"\\<s\\>\" pour indiquer le commencement de la phrase.\n",
    "\n",
    "- Par exemple, dans un modèle trigramme (N=3), une séquence avec deux jetons de début \"\\<s\\>\\<s\\>\" devrait prédire le premier mot d'une phrase. Ainsi, si la phrase est \"J'aime la nourriture\", modifiez-la pour devenir \"\\<s\\>\\<s\\> J'aime la nourriture\".\n",
    "- Ajoutez aussi un jeton de fin \"\\<e\\>\" pour que le modèle puisse prédire quand terminer une phrase.\n",
    "    \n",
    "\n",
    "Dans cette implémentation, vous devez stocker les occurrences des n-grammes sous forme de dictionnaire.\n",
    "\n",
    "- La clé de chaque paire clé-valeur dans le dictionnaire est un tuple de n mots (et non une liste).\n",
    "- La valeur dans la paire clé-valeur est le nombre d'occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n",
    "    n_grams = {}\n",
    "\n",
    "    for sentence in data:\n",
    "        \n",
    "        sentence = [start_token] * n + sentence + [end_token]\n",
    "\n",
    "        for i in range((len(sentence) - (n-1) )): # complete this line\n",
    "\n",
    "            n_gram = tuple(sentence[i:i+n])\n",
    "            \n",
    "            if n_gram in n_grams.keys():\n",
    "            \n",
    "                n_grams[n_gram] += 1\n",
    "            else:\n",
    "                n_grams[n_gram] = 1\n",
    "    \n",
    "\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrammes:\n",
      "{('<s>',): 2, ('i',): 1, ('have',): 1, ('a',): 1, ('mouse',): 2, ('<e>',): 2, ('this',): 1, ('likes',): 1, ('cats',): 1}\n",
      "Bigrammes:\n",
      "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'have'): 1, ('have', 'a'): 1, ('a', 'mouse'): 1, ('mouse', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'mouse'): 1, ('mouse', 'likes'): 1, ('likes', 'cats'): 1, ('cats', '<e>'): 1}\n"
     ]
    }
   ],
   "source": [
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "print(\"Unigrammes:\")\n",
    "print(count_n_grams(sentences, 1))\n",
    "print(\"Bigrammes:\")\n",
    "print(count_n_grams(sentences, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sortie attendue:\n",
    "\n",
    "```CPP\n",
    "Unigrammes:\n",
    "{('<s>',): 2, ('i',): 1, ('have',): 1, ('a',): 1, ('mouse',): 2, ('<e>',): 2, ('this',): 1, ('likes',): 1, ('cats',): 1}\n",
    "Bigrammes:\n",
    "{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'have'): 1, ('have', 'a'): 1, ('a', 'mouse'): 1, ('mouse', '<e>'): 1, ('<s>', 'this'): 1, ('this', 'mouse'): 1, ('mouse', 'likes'): 1, ('likes', 'cats'): 1, ('cats', '<e>'): 1}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 Estimé du maximum de vraisemblance MLE (4 points)\n",
    "\n",
    "\n",
    "Ensuite, estimez la probabilité d'un mot étant donnés les 'n' mots précédents avec les fréquences obtenues.\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_t)}{C(w_{t-1}\\dots w_{t-n})} \\tag{2}$$ \n",
    "\n",
    "\n",
    "La fonction prend en entrée: \n",
    "\n",
    "- word : le mot dont on veut estimer la probabilité\n",
    "- previous_n_gram : le n-gramme précédent, sous forme de tuple\n",
    "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
    "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "    \n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts  else 0\n",
    "\n",
    "    n_plus1_gram = previous_n_gram + (word,)\n",
    "\n",
    "    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts  else 0\n",
    "    \n",
    "    probability = n_plus1_gram_count / previous_n_gram_count\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Quel est le problème de cette fonction? Quelle embûche pourrait-on rencontrer? Répondre avec un exemple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Entrez votre réponse ici*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.3'></a>\n",
    "### 2.3  Lissage add-k (4 points)\n",
    "\n",
    "Vous allez maintenant modifier votre fonction précédente en utilisant le lissage add-k.\n",
    "\n",
    "$$ \\hat{P}(w_t | w_{t-1}\\dots w_{t-n}) = \\frac{C(w_{t-1}\\dots w_{t-n}, w_n) + k}{C(w_{t-1}\\dots w_{t-n}) + k|V|} \\tag{3} $$\n",
    "\n",
    "Recodez la fonction au numéro 2.2 en ajoutant une constante de lissage $k$ and la taille du vocabulaire en paramètres supplémentaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def estimate_probability_smoothing(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "\n",
    "    n = len(next(iter(n_gram_counts)))\n",
    "    \n",
    "    context = tuple(previous_n_gram)\n",
    "    context_count = n_gram_counts[context] if context in n_gram_counts else 0\n",
    "    denominator = context_count + k * vocabulary_size\n",
    "\n",
    "    word_with_context = context + (word,)\n",
    "    word_with_context_count = n_plus1_gram_counts[word_with_context] if word_with_context in n_plus1_gram_counts  else 0\n",
    "    numerator = word_with_context_count + k\n",
    "    \n",
    "    probability = numerator / denominator\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " La probabilité de 'have' étant donné le mot précédent 'i' est: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "tmp_prob = estimate_probability_smoothing(\"have\", ['<s>', 'i'], bigram_counts, trigram_counts, len(unique_words), k=1)\n",
    "\n",
    "print(f\" La probabilité de 'have' étant donné le mot précédent 'i' est: {tmp_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "\n",
    "```CPP\n",
    " La probabilité de 'have' étant donné le mot précédent 'i' est: 0.2500\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.4'></a>\n",
    "### 2.4 Calcul des probabilités des n-grammes\n",
    "\n",
    "#### 2.4.1. Estimation des probabilités (4 points) \n",
    "Complétez la fonction estimate_probabilities qui calcule pour chaque mot du vocabulaire la probabilité d'être généré en utilisant la fonction avec lissage add-k. \n",
    "\n",
    "N'oubliez pas d'ajouter le jetons spécial \"\\<e\\>\" au vocabulaire\n",
    "\n",
    "Cette fonction prends en entrée:\n",
    "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
    "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
    "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n",
    "- vocabulary: l'ensemble du vocabulaire\n",
    "- k: la constante de lissage\n",
    "\n",
    "La fonction retourne un dictionnaire ayant pour clés tous les mots du vocabulaire ainsi que leur probabilité d'être générés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "\n",
    "\n",
    "    previous_n_gram = tuple(previous_n_gram)\n",
    "    \n",
    "\n",
    "    vocabulary = vocabulary + [\"<e>\"]\n",
    "    \n",
    "    probabilities = {}\n",
    "    for word in vocabulary:\n",
    "        probability = estimate_probability_smoothing(word, previous_n_gram, \n",
    "                                           n_gram_counts, n_plus1_gram_counts, \n",
    "                                           len(vocabulary), k=k)\n",
    "        probabilities[word] = probability\n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'likes': 0.1111111111111111,\n",
       " 'a': 0.1111111111111111,\n",
       " 'cats': 0.1111111111111111,\n",
       " 'i': 0.1111111111111111,\n",
       " 'have': 0.1111111111111111,\n",
       " 'this': 0.1111111111111111,\n",
       " 'mouse': 0.2222222222222222,\n",
       " '<e>': 0.1111111111111111}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test \n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "estimate_probabilities([\"a\"], unigram_counts, bigram_counts, unique_words, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "\n",
    "```CPP\n",
    "{'likes': 0.1111111111111111,\n",
    " 'have': 0.1111111111111111,\n",
    " 'this': 0.1111111111111111,\n",
    " 'i': 0.1111111111111111,\n",
    " 'mouse': 0.2222222222222222,\n",
    " 'a': 0.1111111111111111,\n",
    " 'cats': 0.1111111111111111,\n",
    " '<e>': 0.1111111111111111}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2. Probabilités étant donné un contexte (3 points)\n",
    "\n",
    "Affichez maintenant les probabilités des tri-grammes étant donné le context \"i will\" en utilisant les données d'entraînement . N'affichez que les 10 mots les plus probables en ordre décroissant de probabilité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tell', 0.0070981916511745815),\n",
       " ('fix', 0.005239141456819334),\n",
       " ('fight', 0.005239141456819334),\n",
       " ('be', 0.0050701368936961295),\n",
       " ('never', 0.004225114078080108),\n",
       " ('say', 0.003718100388710495),\n",
       " ('not', 0.0025350684468480648),\n",
       " ('ask', 0.0023660638837248605),\n",
       " ('also', 0.0018590501943552475),\n",
       " ('work', 0.001521041068108839)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "bigram_counts_train = count_n_grams(train_data, 2)\n",
    "trigram_counts_train = count_n_grams(train_data, 3)\n",
    "\n",
    "probs_given_and = estimate_probabilities(['i', 'will'], bigram_counts_train, trigram_counts_train, list(voc), k=1)\n",
    "display(sorted(probs_given_and.items(), key=lambda x: 1 - x[1])[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 3. Perplexité (15 points)\n",
    "\n",
    "Dans cette section, vous allez générer le score de perplexité pour évaluer votre modèle sur l'ensemble de test.\n",
    "\n",
    "Pour calculer le score de perplexité de l'ensemble de test sur un modèle n-gramme, utilisez :\n",
    "\n",
    "$$PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} } \\tag{4.1}$$\n",
    "\n",
    "Plus les probabilités sont élevées, plus la perplexité sera basse. \n",
    "\n",
    "### 3.1. Calcul de la perplexité (4 points)\n",
    "Complétez la fonction `calculate_perplexity`, qui pour une phrase donnée, nous donne le score de perplexité. Cette fonction prend en entrée:\n",
    "\n",
    "\n",
    "- sentence: La phrase pour laquelle vous devez calculer la perplexité\n",
    "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
    "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n",
    "- vocabulary_size: la taille du vocabulaire\n",
    "- k: la constante de lissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "   \n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    N = len(sentence) + 1\n",
    "    \n",
    "    sentence = [\"<s>\"] * n + sentence + [\"<e>\"]\n",
    "    \n",
    "    sentence = tuple(sentence)\n",
    "    \n",
    "    product_pi = 1.0\n",
    "    \n",
    "    for t in range(n, len(sentence)): \n",
    "        \n",
    "        n_gram = sentence[t-n:t]\n",
    "        \n",
    "        word = sentence[t]    \n",
    "        \n",
    "        probability = estimate_probability_smoothing(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=k)\n",
    "        product_pi *= (1 / probability)**(1/N)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    return product_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexité de la première phrase: 4.1930\n"
     ]
    }
   ],
   "source": [
    "# test \n",
    "\n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "\n",
    "perplexity = calculate_perplexity(sentences[0],\n",
    "                                         unigram_counts, bigram_counts,\n",
    "                                         len(unique_words), k=1.0)\n",
    "print(f\"Perplexité de la première phrase: {perplexity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Perplexité sur une phrase d'entraînement (4 points)\n",
    "Calculez et affichez la perplexité des modèles bi-grammes, tri-grammes et quadri-grammes à l'aide de votre fonction `calculate_perplexity` définie plus haut sur la première phrase de votre corpus d'entraînement. Utilisez K=0.01 ici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexité du modele 2-gramme : 71.9584\n",
      "Perplexité du modele 3-gramme : 50.7269\n",
      "Perplexité du modele 4-gramme : 50.6769\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unigram_counts_train = count_n_grams(train_data, 1)\n",
    "bigram_counts_train = count_n_grams(train_data, 2)\n",
    "trigram_counts_train = count_n_grams(train_data, 3)\n",
    "quadgram_counts_train = count_n_grams(train_data, 4)\n",
    "#qintgram_counts_train = count_n_grams(train_data, 5)\n",
    "\n",
    "perplexity_bigram = calculate_perplexity(train_data[0],\n",
    "                                         unigram_counts_train, bigram_counts_train,\n",
    "                                         len(voc), k=0.01)\n",
    "print(f\"Perplexité du modele 2-gramme : {perplexity_bigram:.4f}\")\n",
    "\n",
    "perplexity_trigram = calculate_perplexity(train_data[0],\n",
    "                                         bigram_counts_train, trigram_counts_train,\n",
    "                                         len(voc), k=0.01)\n",
    "print(f\"Perplexité du modele 3-gramme : {perplexity_trigram:.4f}\")\n",
    "\n",
    "perplexity_quadgram = calculate_perplexity(train_data[0],\n",
    "                                         trigram_counts_train, quadgram_counts_train,\n",
    "                                         len(voc), k=0.01)\n",
    "print(f\"Perplexité du modele 4-gramme : {perplexity_quadgram:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Perplexité du corpus de test\n",
    "\n",
    "#### 3.3.1. Vous pouvez maintenant calculer et afficher la perplexité des modèles bi-grammes, tri-grammes et quadri-grammes sur votre corpus de test. K=1 ici. (4 points)\n",
    "\n",
    "Pour calculer la perplexité d'un corpus de *m* phrases, il suffit de suivre la formule suivante : \n",
    "\n",
    "Soit *N* le nombre total de jetons dans le corpus de test C.\n",
    "\n",
    "$$Perplexity(C) = \\Big(\\frac{1}{P(s_1, ..., s_m)}\\Big)^{1/N}$$ \n",
    "$$P(s_1, ..., s_m) = \\prod_{i=1}^{m} p(s_i)$$\n",
    "$$p(s_i) = \\prod_{t=1}^{n} \\hat{P}(w_t | w_{t-1}\\dots w_{t-n})$$\n",
    "\n",
    "Puisqu'il s'agit d'un multiplication de probabilités (situées entre 0 et 1), le produit devient nul très rapidement. C'est pourquoi il est plus efficace d'effectuer une transformation vers un espace logarithmique pour transformer les multiplications en addition. Cela donne ainsi la formule suivante:\n",
    "\n",
    "$$LogPerplexity(C) = 2^{-\\frac{1}{N} \\sum_{k=1}^{m} log_{2} \\; p(s_k)}$$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity_corpus(corpus, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "    \n",
    "    sum_sentence = 0.0\n",
    "    N = 0\n",
    "    for sentence in corpus:\n",
    "        sentence = [\"<s>\"] * (n) + sentence + [\"<e>\"]\n",
    "        N += len(sentence) - n\n",
    "\n",
    "        for t in range(n, len(sentence)): # complete this line\n",
    "            \n",
    "            n_gram = sentence[t-n:t]\n",
    "            word = sentence[t]\n",
    "\n",
    "            probability = estimate_probability_smoothing(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=k)\n",
    "            \n",
    "            sum_sentence += math.log2(probability)\n",
    "    \n",
    "    perplexity = 2 ** ((-1 / N) * sum_sentence)\n",
    "    \n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexité du corpus de test:  7.708920690856638\n"
     ]
    }
   ],
   "source": [
    "n_gram_counts = {('<s>', 'quick'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
    "n_plus1_gram_counts = { ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
    "\n",
    "train_corpus = [[\"the\", \"quick\", \"brown\", \"fox\"], [\"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]]\n",
    "n_gram_counts = {('<s>', '<s>'): 2, ('<s>', 'the'): 1, ('<s>', 'jumps'): 1, ('the', 'quick'): 1, ('quick', 'brown'): 1, ('brown', 'fox'): 1, ('fox', '<e>'): 1, ('jumps', 'over'): 1, ('over', 'the'): 1, ('the', 'lazy'): 1, ('lazy', 'dog'): 1, ('dog', '<e>'): 1}\n",
    "n_plus1_gram_counts = {('<s>', '<s>', '<s>', ): 2, ('<s>', '<s>', 'the', ): 1, ('<s>', 'the', 'quick'): 1,  ('<s>', '<s>', 'jumps', ): 1, ('<s>', 'jumps', 'over'): 1, ('the', 'quick', 'brown'): 1, ('quick', 'brown', 'fox'): 1, ('brown', 'fox', '<e>'): 1, ('jumps', 'over', 'the'): 1, ('over', 'the', 'lazy'): 1, ('the', 'lazy', 'dog'): 1, ('lazy', 'dog', '<e>'): 1}\n",
    "\n",
    "vocabulary = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\", \"<e>\"]\n",
    "\n",
    "N = 4\n",
    "V = len(vocabulary)\n",
    "\n",
    "test_corpus = [[\"the\", \"fox\"], [\"jumps\"]]\n",
    "\n",
    "# pour k=1\n",
    "p_the = math.log2((1. + 1.)/(2. + 1.* V))   # prob. pour \"the\" avec ('<s>', '<s>')\n",
    "p_fox = math.log2((0. + 1.)/(1. + 1.* V))   # prob. pour \"fox\" avec ('<s>', 'the')\n",
    "p_e = math.log2((0. + 1.)/(0. + 1.* V))     # prob. pour \"<e>\" avec ('the', 'fox')\n",
    "\n",
    "p_jumps = math.log2((1. + 1.)/(2. + 1.* V)) # prob. pour \"jumps\" avec ('<s>', '<s>')\n",
    "p_e2 = math.log2((0. + 1.)/(1. + 1.* V))    # prob. pour \"<e>\" avec ('<s>', 'jumps')\n",
    "\n",
    "\n",
    "\n",
    "expected_perplexity = p_the + p_fox + p_e + p_jumps + p_e2\n",
    "\n",
    "expected_perplexity =  2 ** ((-1 / N) * expected_perplexity)\n",
    "\n",
    "\n",
    "# Call the calculate_perplexity_corpus function\n",
    "calculated_perplexity = calculate_perplexity_corpus(test_corpus, n_gram_counts, n_plus1_gram_counts, len(vocabulary), k=1.0)\n",
    "\n",
    "print(\"Perplexité du corpus de test: \", calculated_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sortie attendue\n",
    "\n",
    "    Perplexité du corpus de test:  7.708920690856638"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexité du modèle 2-gramme : 98.9474\n",
      "Perplexité du modèle 3-gramme : 208.9466\n",
      "Perplexité du modèle 4-gramme : 489.4859\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "max_grams = 4\n",
    "grams = [count_n_grams(train_data, i) for i in range(1, max_grams+1)]\n",
    "\n",
    "for i in range(len(grams) - 1):\n",
    "\n",
    "    n_gram = grams[i]\n",
    "    n_plus1_gram = grams[i+1]\n",
    "    perplexity = calculate_perplexity_corpus(test_data, n_gram, n_plus1_gram, len(voc), k=0.01)\n",
    "    \n",
    "    print(f'Perplexité du modèle {i + 2}-gramme : {perplexity:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2. Les perplexités attendues peuvent sembler contre-intuitives.  Comparez-les aux perplexités obtenues sur l'ensemble d'entrainement pour les mêmes modèles. Comment expliquez-vous ces résultats et quelle est votre conclusion ?  (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *Entrez votre réponse ici*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 4. Construction d'un modèle d'autocomplétion (15 points)\n",
    "\n",
    "Dans cette dernière partie, vous allez utiliser les modèles n-grammes construits aux numéros précédents afin de faire un modèle d'autocomplétion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5.1'></a>\n",
    "### 4.1 Suggestion d'un mot à partir d'un préfixe (5 points)\n",
    "\n",
    "\n",
    "La première étape sera de construire une fonction qui suggère un mot à partir des premiers caractères entrés par un utilisateur, considérant un seul type de n-gramme.  \n",
    "\n",
    "Complétez la fonction `suggest_word` qui calcule les probabilités pour tous les mots suivants possibles et suggère le mot le plus probable. Comme contrainte supplémentaire, le mot suggéré doit commencer avec le préfixe passé en paramètre. Utilisez vos fonctions provenant du numéro 2. (Modèle n-gramme de mots) pour faire vos prédictions.\n",
    "\n",
    "Cette fonction prends en paramètre:\n",
    "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
    "- n_gram_counts: Un dictionnaire où la clé est le n-gramme et la valeur est la fréquence de ce n-gramme.\n",
    "- n_plus1_gram_counts: Un autre dictionnaire, que vous utiliserez pour trouver la fréquence du n-gramme précédent plus le mot actuel.\n",
    "- vocabulary_size: la taille du vocabulaire\n",
    "- k: la constante de lissage\n",
    "- prefixe: Le début du mot que l'on veut prédire\n",
    "\n",
    "Elle retourne le mot le plus probable avec la probabilité associée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from random import choices\n",
    "\n",
    "def suggest_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0, prefixe=\"\"):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    word_probs = [(word,prob) for word, prob in probabilities.items() if word.startswith(prefixe)]\n",
    "    \n",
    "    if len(word_probs) == 0:\n",
    "        return prefixe, 1\n",
    "    \n",
    "    words, probs = zip(*word_probs)\n",
    "    \n",
    "    best_word_index = np.argmax(probs)\n",
    "    \n",
    "    return words[best_word_index], probs[best_word_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " avec les mots précédents 'i have',\n",
      "\t le mot suggéré est `a` avec la probabilité 0.2222\n",
      "\n",
      "avec les mots précédents 'i have', et une suggestion qui commence par `m`\n",
      "\t le mot suggéré est : `mouse` avec une probabilité de 0.1111\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "\n",
    "previous_tokens = [\"i\", \"have\"]\n",
    "tmp_suggest1 = suggest_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\n",
    "print(f\" avec les mots précédents 'i have',\\n\\t le mot suggéré est `{tmp_suggest1[0]}` avec la probabilité {tmp_suggest1[1]:.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "tmp_starts_with = 'm'\n",
    "tmp_suggest2 = suggest_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, prefixe=tmp_starts_with)\n",
    "print(f\"avec les mots précédents 'i have', et une suggestion qui commence par `{tmp_starts_with}`\\n\\t le mot suggéré est : `{tmp_suggest2[0]}` avec une probabilité de {tmp_suggest2[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sortie attendue\n",
    "\n",
    "```CPP\n",
    "avec les mots précédents 'i have',\n",
    "\t le mot suggéré est `a` avec la probabilité 0.2222\n",
    "\n",
    "avec les mots précédents 'i have', et une suggestion qui commence par `m`\n",
    "\t le mot suggéré est : `mouse` avec une probabilité de 0.1111\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5.2'></a>\n",
    "### 4.2 Suggestions multiples (5 points)\n",
    "\n",
    "Afin de suggérer plusieurs mots à l'utilisateur, une stratégie que l'on peut utiliser est de retourner un ensemble de mots suggérés par plusieurs types de modèles n-grammes.\n",
    "\n",
    "En utilisant la fonction `suggest_word` du numéro précédent, complétez la fonction `get_suggestions` qui retourne les suggestions des modèles n-grammes passés en paramètre. Vous devrez aussi enlever les doublons dans les suggestions s'il y en a, et ordonner la liste des suggestions en commençant par le mot ayant la probabilité la plus élevée.\n",
    "\n",
    "La fonction get_suggestions prends en paramètres:\n",
    "- previous_n_gram: le n-gramme précédent, sous forme de tuple\n",
    "- n_gram_counts_list: une liste de n-grammes dans l'ordre suivant [unigrammes, bigrammes, trigrammes, quadrigrammes, ...]\n",
    "- vocabulary_size: la taille du vocabulaire\n",
    "- k: la constante de lissage (entre 0 et 1)\n",
    "- prefixe: Le début du mot que l'on veut prédire, \"\" si au aucun préfixe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, prefixe=\"\"):\n",
    "    model_counts = len(n_gram_counts_list)\n",
    "    suggestions = []\n",
    "    for i in range(model_counts-1):\n",
    "        n_gram_counts = n_gram_counts_list[i]\n",
    "        n_plus1_gram_counts = n_gram_counts_list[i+1]\n",
    "\n",
    "        # Making sure the output is valid given the n-gram\n",
    "        if len(previous_tokens) - 2 >= i:\n",
    "            # If input is longer than size of n-gram, reduce it\n",
    "            formatted_previous = previous_tokens[-(max(0, i+1)):]\n",
    "        else:\n",
    "            # If input is shorter, add <s> token\n",
    "            formatted_previous = ['<s>'] * max(0, i - len(previous_tokens) + 1) + previous_tokens\n",
    "        \n",
    "        suggestion = suggest_word(formatted_previous, n_gram_counts,\n",
    "                                    n_plus1_gram_counts, vocabulary,\n",
    "                                    k=k, prefixe=prefixe)\n",
    "        suggestions.append(suggestion)\n",
    "    \n",
    "    suggestions = list(map(lambda x: x[0] , sorted(suggestions, key=lambda x: -x[1])))\n",
    "    return list(set(suggestions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etant donné les mots i have, je suggère :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test \n",
    "sentences = [['i', 'have', 'a', 'mouse'],\n",
    "             ['this', 'mouse', 'likes', 'cats']]\n",
    "unique_words = list(set(sentences[0] + sentences[1]))\n",
    "\n",
    "unigram_counts = count_n_grams(sentences, 1)\n",
    "bigram_counts = count_n_grams(sentences, 2)\n",
    "trigram_counts = count_n_grams(sentences, 3)\n",
    "quadgram_counts = count_n_grams(sentences, 4)\n",
    "qintgram_counts = count_n_grams(sentences, 5)\n",
    "\n",
    "n_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\n",
    "previous_tokens = [\"i\", \"have\"]\n",
    "tmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n",
    "\n",
    "print(f\"Etant donné les mots i have, je suggère :\")\n",
    "display(tmp_suggest3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sortie attendue\n",
    "\n",
    "```CPP \n",
    "Etant donné les mots i have, je suggère :\n",
    "['a']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5.3'></a>\n",
    "### 4.3 Autocomplétion (5 points)\n",
    "\n",
    "Il est maintenant temps de combiner vos fonctions afin de créer le modèle d'autocomplétion. En utilisant le jeu de données d'entraînement, calculez la fréquence des n-grammes allant de 1 à 5 et utilisez la fonction *get_suggestions* afin de suggérer des mots. Vous devrez être en mesure de toujours suggérer des mots à partir du dernier mot entré par l'utilisateur.\n",
    "\n",
    "Complétez la fonction *update_suggestions*:\n",
    "- la variable texte_actuel contient tout le texte entré par l'utilisateur\n",
    "- la variable top_suggestions contient les suggestions qui seront proposées\n",
    "\n",
    "Vous devrez changer le contenu de la variable top_suggestions pour qu'elle contienne les suggestions des n-grammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = count_n_grams(train_data, 1)\n",
    "bigram_counts = count_n_grams(train_data, 2)\n",
    "trigram_counts = count_n_grams(train_data, 3)\n",
    "quadgram_counts = count_n_grams(train_data, 4)\n",
    "qintgram_counts = count_n_grams(train_data, 5)\n",
    "\n",
    "n_gram_counts_list_trained = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb78b779e3a047c2ae98da9d41a104e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Entrez votre text ici...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e677d75d97754fe4a6125b2194932606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Suggestions: ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "text_input = widgets.Text(placeholder=\"Entrez votre text ici...\")\n",
    "\n",
    "suggestions_label = widgets.Label(value=\"Suggestions: \")\n",
    "\n",
    "def update_suggestions(change):\n",
    "    texte_actuel = change[\"new\"].lower()\n",
    "    \n",
    "    prefixe = texte_actuel.split(\" \")[-1]\n",
    "    previous_tokens = texte_actuel.split(\" \")[:-1]\n",
    "\n",
    "    top_suggestions = get_suggestions(previous_tokens, n_gram_counts_list_trained, list(voc), k=0.01, prefixe=prefixe)\n",
    "    suggestions_label.value = \"Suggestions: \" + \", \".join(top_suggestions)\n",
    "\n",
    "text_input.observe(update_suggestions, names=\"value\")\n",
    "\n",
    "display(text_input)\n",
    "display(suggestions_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "## 5. Modèle de génération de phrases (30 points)\n",
    "\n",
    "Il est aussi possible de construire un modèle de génération de phrases avec les modèles n-grammes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dans la cadre d'un modèle de génération de phrases, indiquez pourquoi la stratégie de suggestion des mots en 5.1 ne peut pas fonctionner ? (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">La fonction *suggest_word*, retourne le mot ayant la plus grande probabilité d'être généré. nous allons toujours nous retrouver avec la même phrase générée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6.1'></a>\n",
    "\n",
    "### 5.1 Génération stochastique de mots (5 points)\n",
    "\n",
    "Recodez la fonction suggest_word afin d'utiliser une suggestion stochastique. Autrement dit, au lieu de retourner le mot le plus probable, vous devrez générez le mot suivant selon sa probabilité.\n",
    "\n",
    "Par exemple si le mot 'like' a la probabilité 0.25 d'être généré, alors il sera retourné 25% du temps.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from random import choices\n",
    "\n",
    "def suggest_word_with_probs(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "    \n",
    "    n = len(list(n_gram_counts.keys())[0]) \n",
    "\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    word_probs = [(word,prob) for word, prob in probabilities.items()]\n",
    "    \n",
    "    \n",
    "    words, probs = zip(*word_probs)\n",
    "    \n",
    "    choice = choices(word_probs, probs)[0]\n",
    "\n",
    "    return choice[0], choice[1] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6.2'></a>\n",
    "### 5.2 Générations de phrases (10 points)\n",
    "\n",
    "#### 5.2.1. Génération stochastique (4 points)\n",
    "Complétez maintenant la fonction `generate_sentence` qui génère une phrase longue de n_words en appelant votre nouvelle fonction `suggest_words_with_probs`. La génération doit s'arrêter si le modèle génère un jeton de fin de phrase.\n",
    "\n",
    "Il ne faut pas oublier d'initialiser les phrases à générer avec le bon nombre de jetons de début de phrase (`<s>`). Par exemple, s'il s'agit d'un modèle bigramme, il faudra initialiser la phrase à [`<s>`]. S'il s'agit d'un modèle trigramme, il faudra initialiser la phrase à [`<s>`, `<s>`]. Vous pouvez trouver la taille du contexte à l'aide de l'expression suivante `len(next(iter(n_gram_counts)))`.\n",
    "\n",
    "Ensuite, il faudra passer à la fonction `suggest_word` les `n` derniers mots générés où `n` correspond à la taille du contexte.\n",
    "Finalement, il faudra arrêter la génération si le jeton généré est le jeton de fin (`<e>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.0001):\n",
    "    context_length = len(next(iter(n_gram_counts)))\n",
    "    sentence = ['<s>'] * (context_length)\n",
    "    context = sentence\n",
    "    for i in range(n_words):\n",
    "\n",
    "        suggestion = suggest_word_with_probs(context, n_gram_counts,\n",
    "                                n_plus1_gram_counts, vocabulary,\n",
    "                                k=k)\n",
    "        sentence.append(suggestion[0])\n",
    "        context = sentence[-context_length:]\n",
    "        if sentence[-1] == \"<e>\":\n",
    "            break\n",
    "\n",
    "    return \" \".join(list(sentence[context_length:-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2. Test sur des n-grammes (2 points)\n",
    "Testez ensuite votre fonction avec des trigrammes et des 5-grammes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-grammes : \n",
      "\n",
      "\n",
      "youre raped theory involves point really behalf unfairness radical recommendations barely lose worked removal\n",
      "its jackport lingering powered eliminate gun okeechobee ground ends designed super pledges better clintons\n",
      "amount work kids hose sorry decisions collusion screening explanation brian yearning encouraged before credence\n",
      "what by himself badlydepleted demonstrators outsider parttime priorities divider wont vile selling somali guard\n",
      "you expect assimilation bond keeps dads award tougher status desire dallas singapore ballroom minds\n",
      "reptile breakfast billions personal neighbors deliver closed don finisher halt products strongly spin admirals\n",
      "you walk lone sitting farms taxation beneficiary minnesota interested unborn costing prepping class reaching\n",
      "dont 77000 income amounts fierce overdue uphold graham restore eternal bleeding unlock nascar sicko\n",
      "gon lie 150 consider los ally warm quoted elect multiple pilot entry fest welders\n",
      "sadly hey thered mvp gates paul perfectly rally 300000 successful reimbursed language allegations puppet\n"
     ]
    }
   ],
   "source": [
    "print('5-grammes : \\n\\n')\n",
    "for i in range(10):\n",
    "    print(generate_sentence(15, quadgram_counts, qintgram_counts, list(voc), k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-grammes : \n",
      "\n",
      "\n",
      "are everyday tour general withdraw served employers burglary walls\n",
      "its a terrible terrible breeding ground\n",
      "all my life\n",
      "right right\n",
      "its amazing what percentage of very very well and\n",
      "get out theyre getting massive amounts of money to\n",
      "i am with you i ran against him\n",
      "the middle east\n",
      "because they know it yet but theyre going to\n",
      "shed taxes whenever beyond good sleep brave capability came\n"
     ]
    }
   ],
   "source": [
    "print('3-grammes : \\n\\n')\n",
    "\n",
    "for i in range(10):\n",
    "    print(generate_sentence(10, bigram_counts, trigram_counts, list(voc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3. Avec k=1.0, que se passe-t-il avec les phrases générées et quelle en est la raison principale ? Que pouvez-vous faire pour améliorer la situation? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Les phrases ne sont pas vraiment cohérentes avec k=1.0 car on a trop transferré de masse de probabilité des événements fréquents vers les événements rares. \n",
    "On peut adopter un lissage moins brutal et descendre la valeur de k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4.  Quels sont les problèmes si la constante k a une valeur trop petite, voir 0?  (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Avec k=0, on revient à un calcul de l'estimé de maximum de vraisemblance, ce qui entraine des possibilités de probabilités à 0 si le corpus de test contient des n-grammes inconnus et une incapacité donc à calculer la perplexité du modèle n-gramme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5.3'></a>\n",
    "### 5.3. Amélioration de la génération stochastique de mots (12 points)\n",
    "\n",
    "#### 5.3.1. Amélioration stochastique \n",
    "\n",
    "Comme vous avez pu l'observer, la génération stochastique, bien qu'elle soit efficace pour générer des phrases différentes, a tendance à ne pas générer des phrases toujours cohérentes. Proposez une amélioration de la méthode `suggest_word` que vous implémenterez dans la méthode `suggest_word_new` permettant de générer des phrases plus cohérentes. \n",
    "\n",
    "##### Décrivez votre méthode dans la cellule suivante (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Il existe plusieurs implémentations possibles. Par exemple, ils peuvent utiliser les 10 mots les plus probables. Sinon, ils peuvent faire du backoff lorsque les probabilités sont nulles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implémentez la méthode proposée (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from random import choices\n",
    "\n",
    "def suggest_word_new(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, k=1.0):\n",
    "\n",
    "    n = len(list(n_gram_counts.keys())[0])\n",
    "\n",
    "    previous_n_gram = previous_tokens[-n:]\n",
    "    \n",
    "    probabilities = estimate_probabilities(previous_n_gram,\n",
    "                                           n_gram_counts, n_plus1_gram_counts,\n",
    "                                           vocabulary, k=k)\n",
    "    word_probs = [(word,prob) for word, prob in probabilities.items()]\n",
    "    words, probs = zip(*word_probs)\n",
    "\n",
    "    best_choices = np.argsort(probs)[-10:]\n",
    "\n",
    "    best_words = []\n",
    "    best_probs = []\n",
    "    for i in best_choices:\n",
    "        best_words.append(words[i])\n",
    "        best_probs.append(probs[i])\n",
    "\n",
    "    choice = choices(best_words, best_probs)\n",
    "\n",
    "    return choice[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2. Génération améliorée (2 points)\n",
    "Recodez maintenant la fonction `generate_sentence_new` pour appeler votre nouvelle méthode `suggest_word_new`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_new(n_words, n_gram_counts, n_plus1_gram_counts, vocabulary, k=0.001):\n",
    "    context_length = len(next(iter(n_gram_counts)))\n",
    "    sentence = ['<s>'] * (context_length)\n",
    "    context = sentence\n",
    "    for i in range(n_words):\n",
    "\n",
    "        suggestion = suggest_word_new(context, n_gram_counts,\n",
    "                                n_plus1_gram_counts, vocabulary,\n",
    "                                k=k)\n",
    "        sentence.append(suggestion)\n",
    "        context = sentence[-context_length:]\n",
    "        if sentence[-1] == \"<e>\":\n",
    "            break\n",
    "\n",
    "    return \" \".join(list(sentence[context_length:-1]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3. Test sur des n-grammes (2 points)\n",
    "Testez ensuite votre fonction avec des 3-grammes et des 5-grammes et validez que les phrases sont plus cohérentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-grammes : \n",
      "\n",
      "\n",
      "its just as good but you need good strong hands\n",
      "and i have to say the republican national convention was such a tremendous success\n",
      "and its getting bigger very fast\n",
      "its time we stop\n",
      "were going have the wall\n",
      "and i have a very special message for you tonight\n",
      "but you could have picked a liberal person and we understand it we could\n",
      "and i have to do with it\n",
      "and i said to her last night i said to her yesterday i said\n",
      "and i have to tell you\n"
     ]
    }
   ],
   "source": [
    "print('5-grammes : \\n\\n')\n",
    "for i in range(10):\n",
    "    print(generate_sentence_new(15, quadgram_counts, qintgram_counts, list(voc), k=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-grammes : \n",
      "\n",
      "\n",
      "you dont know if hes here\n",
      "so we got to get in\n",
      "but they say well ok maybe because i know so well know\n",
      "and i will fight harder for you than anyone theyve ever done it i\n",
      "the national model of providing school choice\n",
      "and i think the biggest crowds in the back of her workrelated emails\n",
      "and by the way were going to be a very good job because our\n",
      "but we have to do is were going to have a government of by\n",
      "were going to do it\n",
      "the other night right near fort bragg\n"
     ]
    }
   ],
   "source": [
    "print('3-grammes : \\n\\n')\n",
    "\n",
    "for i in range(10):\n",
    "    print(generate_sentence_new(15, bigram_counts_train, trigram_counts_train, list(voc), k=0.001))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-3"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
